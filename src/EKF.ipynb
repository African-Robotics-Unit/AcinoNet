{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy import stats\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# from calib.utils import load_scene\n",
    "from lib import utils, calib, plotting\n",
    "from lib.points import get_pairwise_3d_points_from_df\n",
    "from lib.app import reconstruction_reprojection_video, combine_dlc_vids\n",
    "from lib.misc import rot_x, rot_y, rot_z, get_markers\n",
    "\n",
    "ROOT_DATA_DIR = os.path.join(\"..\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction Params.\n",
    "You **must** define these - do not delete.<br>\n",
    "Thereafter, run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = os.path.join(ROOT_DATA_DIR, \"2019_03_09\", \"lily\", \"run\")\n",
    "DATA_DIR = os.path.join(ROOT_DATA_DIR, \"2017_08_29 - Copy\", \"top\", \"jules\", \"run1_1\")\n",
    "\n",
    "start_frame = 20\n",
    "end_frame = 70\n",
    "\n",
    "# DLC p_cutoff - any points with likelihood < dlc_thresh are not trusted in EKF\n",
    "dlc_thresh = 0.8  # change this only if EKF result is unsatisfactory\n",
    "\n",
    "include_lure = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EKF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indices for the states\n",
    "state_id = [\n",
    "    'x_0', 'y_0', 'z_0',         # head position in inertial\n",
    "    'phi_0', 'theta_0', 'psi_0', # head rotation in inertial\n",
    "    'phi_1', 'theta_1', 'psi_1', # neck\n",
    "    'theta_2',                   # front torso\n",
    "    'phi_3', 'theta_3', 'psi_3', # back torso\n",
    "    'theta_4', 'psi_4',          # tail_base\n",
    "    'theta_5', 'psi_5',          # tail_mid\n",
    "    'theta_6', 'theta_7',        # l_shoulder, l_front_knee\n",
    "    'theta_8', 'theta_9',        # r_shoulder, r_front_knee\n",
    "    'theta_10', 'theta_11',      # l_hip, l_back_knee\n",
    "    'theta_12', 'theta_13',      # r_hip, r_back_knee\n",
    "    ]\n",
    "\n",
    "# define DLC labels\n",
    "markers = get_markers(include_lure)\n",
    "\n",
    "if include_lure:\n",
    "    state_id += ['x_l', 'y_l', 'z_l'] # lure position in inertial\n",
    "\n",
    "n_states = 3*len(state_id)\n",
    "vel_idx = n_states//3\n",
    "acc_idx = n_states*2//3\n",
    "n_markers = len(markers)\n",
    "\n",
    "idx = {}\n",
    "for i, deriv_idx in enumerate([0, vel_idx, acc_idx]):\n",
    "    idx.update({'d'*i+state: deriv_idx+j for j, state in enumerate(state_id)})\n",
    "\n",
    "# Load extrinsic params\n",
    "k_arr, d_arr, r_arr, t_arr, cam_res, n_cams, scene_fpath = utils.find_scene_file(DATA_DIR)\n",
    "camera_params = [[K, D, R, T] for K, D, R, T in zip(k_arr, d_arr, r_arr, t_arr)]\n",
    "\n",
    "# other vars\n",
    "start_frame -= 1 # 0 based indexing\n",
    "n_frames = end_frame - start_frame\n",
    "sigma_bound = 3\n",
    "max_pixel_err = cam_res[0] # used in measurement covariance R\n",
    "sT = 1/90 if max_pixel_err == 1920 else 1/120 # timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_marker_coords(x: np.ndarray):\n",
    "    \"\"\"Returns a numpy array of the 3D marker coordinates (shape Nx3) for a given state vector x.\n",
    "    \"\"\"\n",
    "    # 1. Rotations\n",
    "    RI_0 = rot_z(x[idx['psi_0']]) @ rot_x(x[idx['phi_0']]) @ rot_y(x[idx['theta_0']]) # Head\n",
    "    R0_I = RI_0.T\n",
    "    RI_1 = rot_z(x[idx['psi_1']]) @ rot_x(x[idx['phi_1']]) @ rot_y(x[idx['theta_1']]) @ RI_0 # Neck\n",
    "    R1_I = RI_1.T\n",
    "    RI_2 = rot_y(x[idx['theta_2']]) @ RI_1 # Front torso\n",
    "    R2_I = RI_2.T\n",
    "    RI_3 = rot_z(x[idx['psi_3']]) @ rot_x(x[idx['phi_3']]) @ rot_y(x[idx['theta_3']]) @ RI_2 # Back Torso\n",
    "    R3_I = RI_3.T\n",
    "    RI_4 = rot_z(x[idx['psi_4']]) @ rot_y(x[idx['theta_4']]) @ RI_3 # Tail base\n",
    "    R4_I = RI_4.T\n",
    "    RI_5 = rot_z(x[idx['psi_5']]) @ rot_y(x[idx['theta_5']]) @ RI_4 # Tail mid\n",
    "    R5_I = RI_5.T\n",
    "    RI_6 = rot_y(x[idx['theta_6']]) @ RI_2 # l_shoulder\n",
    "    R6_I = RI_6.T\n",
    "    RI_7 = rot_y(x[idx['theta_7']]) @ RI_6 # l_front_knee\n",
    "    R7_I = RI_7.T\n",
    "    RI_8 = rot_y(x[idx['theta_8']]) @ RI_2 # r_shoulder\n",
    "    R8_I = RI_8.T\n",
    "    RI_9 = rot_y(x[idx['theta_9']]) @ RI_8 # r_front_knee\n",
    "    R9_I = RI_9.T\n",
    "    RI_10 = rot_y(x[idx['theta_10']]) @ RI_3 # l_hip\n",
    "    R10_I = RI_10.T\n",
    "    RI_11 = rot_y(x[idx['theta_11']]) @ RI_10 # l_back_knee\n",
    "    R11_I = RI_11.T\n",
    "    RI_12 = rot_y(x[idx['theta_12']]) @ RI_3 # r_hip\n",
    "    R12_I = RI_12.T\n",
    "    RI_13 = rot_y(x[idx['theta_13']]) @ RI_12 # r_back_knee\n",
    "    R13_I = RI_13.T\n",
    "\n",
    "    # 2. Positions\n",
    "    p_head = np.array([x[[idx['x_0'], idx['y_0'], idx['z_0']]]]).T\n",
    "    p_l_eye = p_head + R0_I @ np.array([[0, 0.03, 0]]).T\n",
    "    p_r_eye = p_head + R0_I @ np.array([[0, -0.03, 0]]).T\n",
    "    p_nose = p_head + R0_I @ np.array([[0.055, 0, -0.055]]).T\n",
    "    \n",
    "    p_neck_base = p_head + R1_I @ np.array([[-0.28, 0, 0]]).T\n",
    "    p_spine = p_neck_base + R2_I @ np.array([[-0.37, 0, 0]]).T\n",
    "    \n",
    "    p_tail_base = p_spine + R3_I @ np.array([[-0.37, 0, 0]]).T\n",
    "    p_tail_mid = p_tail_base + R4_I @ np.array([[-0.28, 0, 0]]).T\n",
    "    p_tail_tip = p_tail_mid + R5_I @ np.array([[-0.36, 0, 0]]).T\n",
    "    \n",
    "    p_l_shoulder = p_neck_base + R2_I @ np.array([[-0.04, 0.08, -0.10]]).T\n",
    "    p_l_front_knee = p_l_shoulder + R6_I @ np.array([[0, 0, -0.24]]).T\n",
    "    p_l_front_ankle = p_l_front_knee + R7_I @ np.array([[0, 0, -0.28]]).T\n",
    "    \n",
    "    p_r_shoulder = p_neck_base + R2_I @ np.array([[-0.04, -0.08, -0.10]]).T\n",
    "    p_r_front_knee = p_r_shoulder + R8_I @ np.array([[0, 0, -0.24]]).T\n",
    "    p_r_front_ankle = p_r_front_knee + R9_I @ np.array([[0, 0, -0.28]]).T\n",
    "\n",
    "    p_l_hip = p_tail_base + R3_I @ np.array([[0.12, 0.08, -0.06]]).T\n",
    "    p_l_back_knee = p_l_hip + R10_I @ np.array([[0, 0, -0.32]]).T\n",
    "    p_l_back_ankle = p_l_back_knee + R11_I @ np.array([[0, 0, -0.25]]).T\n",
    "\n",
    "    p_r_hip = p_tail_base + R3_I @ np.array([[0.12, -0.08, -0.06]]).T\n",
    "    p_r_back_knee = p_r_hip + R12_I @ np.array([[0, 0, -0.32]]).T\n",
    "    p_r_back_ankle = p_r_back_knee + R13_I @ np.array([[0, 0, -0.25]]).T\n",
    "\n",
    "    coords_3d = np.array([\n",
    "        p_l_eye, p_r_eye, p_nose,\n",
    "        p_neck_base, p_spine, \n",
    "        p_tail_base, p_tail_mid, p_tail_tip,\n",
    "        p_l_shoulder, p_l_front_knee, p_l_front_ankle,\n",
    "        p_r_shoulder, p_r_front_knee, p_r_front_ankle,\n",
    "        p_l_hip, p_l_back_knee, p_l_back_ankle,\n",
    "        p_r_hip, p_r_back_knee, p_r_back_ankle,\n",
    "        ])\n",
    "\n",
    "    if include_lure:\n",
    "        p_lure = np.array([x[[idx['x_l'], idx['y_l'], idx['z_l']]]]).T\n",
    "        coords_3d = np.concatenate([coords_3d, [p_lure]])\n",
    "    \n",
    "    return np.reshape(coords_3d, (len(coords_3d), 3))\n",
    "\n",
    "\n",
    "def h_function(x: np.ndarray, k: np.ndarray, d: np.ndarray, r: np.ndarray, t: np.ndarray):\n",
    "    \"\"\"Returns a numpy array of the 2D marker pixel coordinates (shape Nx2) for a given state vector x and camera parameters k, d, r, t.\n",
    "    \"\"\"\n",
    "    coords_3d = get_3d_marker_coords(x)\n",
    "    coords_2d = calib.project_points_fisheye(coords_3d, k, d, r, t) # Project the 3D positions to 2D\n",
    "    \n",
    "    return coords_2d\n",
    "\n",
    "\n",
    "def predict_next_state(x: np.ndarray, dt: np.float32):\n",
    "    \"\"\"Returns a numpy array of the predicted states for a given state vector x and time delta dt.\n",
    "    \"\"\"\n",
    "    acc_prediction = x[acc_idx:]\n",
    "    vel_prediction = x[vel_idx:acc_idx] + dt*acc_prediction\n",
    "    pos_prediction = x[:vel_idx] + dt*vel_prediction + (0.5*dt**2)*acc_prediction\n",
    "    \n",
    "    return np.concatenate([pos_prediction, vel_prediction, acc_prediction]).astype(np.float32)\n",
    "\n",
    "\n",
    "def numerical_jacobian(func, x: np.ndarray, *args):\n",
    "    \"\"\"Returns a numerically approximated jacobian of func with respect to x.\n",
    "    Additional parameters will be passed to func using *args in the format: func(*x, *args)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    eps = 1e-3\n",
    "    \n",
    "    fx = func(x, *args).flatten()\n",
    "    xpeturb=x.copy()\n",
    "    jac = np.empty((len(fx), n))\n",
    "    for i in range(n):\n",
    "        xpeturb[i] = xpeturb[i]+eps\n",
    "        jac[:,i] = (func(xpeturb, *args).flatten() - fx)/eps\n",
    "        xpeturb[i]=x[i]\n",
    "        \n",
    "    return jac\n",
    "\n",
    "def save_ekf(states, smooth_states, path, start_frame, lure_included, dlc_thresh):\n",
    "    import scipy.io\n",
    "    types = [\"\", \"_padded\"]\n",
    "    lure_str = \"_with_lure\" if lure_included else \"\"\n",
    "    ekf_path = os.path.join(path, \"ekf\")\n",
    "    os.makedirs(ekf_path, exist_ok=True)\n",
    "    \n",
    "    for t in types:\n",
    "        \n",
    "        if \"padded\" in t:\n",
    "            nan_arr = np.full((start_frame, states.shape[1]), np.nan)\n",
    "\n",
    "            states = np.concatenate((nan_arr, states))\n",
    "            smooth_states = np.concatenate((nan_arr, smooth_states))\n",
    "        \n",
    "        positions = np.array([get_3d_marker_coords(state) for state in states])\n",
    "        smooth_positions = np.array([get_3d_marker_coords(state) for state in smooth_states])\n",
    "        \n",
    "        file_data = dict(\n",
    "            ekf_positions=positions,\n",
    "            smooth_positions=smooth_positions,\n",
    "            ekf_states=states,\n",
    "            smooth_states=smooth_states,\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(ekf_path, f\"ekf{lure_str}{t}.pickle\"), 'wb') as f:\n",
    "            pickle.dump(file_data, f)\n",
    "        \n",
    "        scipy.io.savemat(os.path.join(ekf_path, f'ekf{lure_str}{t}.mat'), file_data)\n",
    "        \n",
    "        if \"padded\" in t:\n",
    "            out_fpath = os.path.join(ekf_path, \"ekf.avi\")\n",
    "            reconstruction_reprojection_video(path, out_fpath, smooth_positions, include_lure=lure_included, dlc_thresh=dlc_thresh)\n",
    "        \n",
    "    print('\\nSaved files to', ekf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DLC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DLC 2D point files (.h5 outputs)\n",
    "dlc_2d_point_files = sorted(glob.glob(os.path.join(DATA_DIR, '*.h5')))\n",
    "assert(len(dlc_2d_point_files) == n_cams), f\"# of dlc '.h5' files != # of cams in {n_cams}_cam_scene_sba.json\"\n",
    "\n",
    "# Load Measurement Data (pixels, likelihood)\n",
    "points_2d_df = utils.load_dlc_points_as_df(dlc_2d_point_files)\n",
    "print(\"DLC points dataframe:\\n\", points_2d_df)\n",
    "\n",
    "# Restructure dataframe\n",
    "points_df = points_2d_df.set_index(['frame', 'camera','marker'])\n",
    "points_df = points_df.stack().unstack(level=1).unstack(level=1).unstack()\n",
    "\n",
    "# Pixels array\n",
    "pixels_df = points_df.loc[:, (range(n_cams), markers, ['x','y'])]\n",
    "pixels_df = pixels_df.reindex(columns=pd.MultiIndex.from_product([range(n_cams), markers, ['x','y']]))\n",
    "pixels_arr = pixels_df.to_numpy() #shape - (n_frames, n_cams * n_markers * 2)\n",
    "\n",
    "# Likelihood array\n",
    "likelihood_df = points_df.loc[:, (range(n_cams), markers, 'likelihood')]\n",
    "likelihood_df = likelihood_df.reindex(columns=pd.MultiIndex.from_product([range(n_cams), markers, ['likelihood']]))\n",
    "likelihood_arr = likelihood_df.to_numpy() #shape - (n_frames, n_cams * n_markers * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise EKF matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d_df = get_pairwise_3d_points_from_df(\n",
    "    points_2d_df[points_2d_df['likelihood']>dlc_thresh], # ignore points with low likelihood\n",
    "    k_arr, d_arr.reshape((-1,4)), r_arr, t_arr,\n",
    "    calib.triangulate_points_fisheye\n",
    ")\n",
    "\n",
    "# estimate initial points\n",
    "nose_pts = points_3d_df[points_3d_df[\"marker\"]==\"nose\"][[\"frame\", \"x\", \"y\", \"z\"]].values\n",
    "nose_x_slope, nose_x_intercept, *_ = stats.linregress(nose_pts[:,0], nose_pts[:,1]) \n",
    "nose_y_slope, nose_y_intercept, *_ = stats.linregress(nose_pts[:,0], nose_pts[:,2])\n",
    "\n",
    "nose_x_est = start_frame*nose_x_slope + nose_x_intercept # initial nose x\n",
    "nose_y_est = start_frame*nose_y_slope + nose_y_intercept # initial nose y\n",
    "nose_psi_est = np.arctan2(nose_y_slope, nose_x_slope)    # initial yaw angle relative to inertial\n",
    "\n",
    "# INITIAL STATES\n",
    "states = np.zeros(n_states)\n",
    "states[[idx['x_0'], idx['y_0'],idx['psi_0']]] = [nose_x_est, nose_y_est, nose_psi_est] # head x, y & psi (yaw) in inertial\n",
    "states[[idx['dx_0'], idx['dy_0']]] = [nose_x_slope/sT, nose_y_slope/sT]                # head x & y velocity in inertial\n",
    "\n",
    "# INITIAL STATE COVARIANCE P - how much do we trust the initial states\n",
    "# acceleration\n",
    "p_lin_acc = np.ones(3)*3**2\n",
    "p_ang_acc = np.ones(22)*3**2\n",
    "p_ang_acc[10:] = 5**2\n",
    "# velocity\n",
    "p_lin_vel = np.ones(3)*5**2          # Know this within 2.5m/s and it's a uniform random variable\n",
    "p_ang_vel = np.ones(22)*3**2\n",
    "# position\n",
    "p_lin_pos = np.ones(3)*3**2          # Know initial position within 4m\n",
    "p_ang_pos = np.ones(22)*(np.pi/4)**2 # Know initial angles within 60 degrees, heading may need to change\n",
    "\n",
    "P_list = [p_lin_pos, p_ang_pos, p_lin_vel, p_ang_vel, p_lin_acc, p_ang_acc]\n",
    "\n",
    "# PROCESS COVARIANCE Q - how \"noisy\" the constant acceleration model is\n",
    "qb_list = [\n",
    "    5.0, 5.0, 5.0,    # head x, y, z in inertial\n",
    "    10.0, 10.0, 10.0, # head phi, theta, psi in inertial\n",
    "    5.0, 25.0, 5.0,   # neck phi, theta, psi\n",
    "    50.0,             # front-torso theta\n",
    "    5.0, 50.0, 25.0,  # back torso phi, theta, psi\n",
    "    100.0, 30.0,      # tail base theta, psi\n",
    "    140.0, 40.0,      # tail mid theta, psi\n",
    "    350.0, 200.0,     # l_shoulder theta, l_front_knee theta\n",
    "    350.0, 200.0,     # r_shoulder theta, r_front_knee theta\n",
    "    450.0, 400.0,     # l_hip theta, l_back_knee theta\n",
    "    450.0, 400.0,     # r_hip theta, r_back_knee theta\n",
    "]\n",
    "\n",
    "if include_lure:\n",
    "    lure_pts = points_3d_df[points_3d_df[\"marker\"]==\"lure\"][[\"frame\", \"x\", \"y\", \"z\"]].values\n",
    "    lure_x_slope, lure_x_intercept, *_ = stats.linregress(lure_pts[:,0], lure_pts[:,1]) \n",
    "    lure_y_slope, lure_y_intercept, *_ = stats.linregress(lure_pts[:,0], lure_pts[:,2])\n",
    "    \n",
    "    lure_x_est = start_frame*lure_x_slope + lure_x_intercept # initial lure x\n",
    "    lure_y_est = start_frame*lure_y_slope + lure_y_intercept # initial lure y\n",
    "    \n",
    "    states[[idx['x_l'], idx['y_l']]] = [lure_x_est, lure_y_est]             # lure x & y in inertial\n",
    "    states[[idx['dx_l'], idx['dy_l']]] = [lure_x_slope/sT, lure_y_slope/sT] # lure x & y velocity in inertial\n",
    "    \n",
    "    p_lure_acc = np.ones(3)*3**2\n",
    "    p_lure_vel = np.ones(3)*5**2 # Know this within 2.5m/s and it's a uniform random variable\n",
    "    p_lure_pos = np.ones(3)*3**2 # Know initial position within 4m\n",
    "    \n",
    "    P_list.insert(2, P_list[0]) # lure pos initial state covariance same as head\n",
    "    P_list.insert(5, P_list[2]) # lure vel initial state covariance same as head\n",
    "    P_list.append(P_list[6])    # lure accel initial state covariance same as head\n",
    "    \n",
    "    qb_list += qb_list[0:3] # lure x, y, z in inertial - same as head\n",
    "\n",
    "P = np.diag(np.concatenate(P_list))\n",
    "\n",
    "qb = (np.diag(qb_list)/2)**2\n",
    "Q = np.block([\n",
    "    [sT**4/4 * qb, sT**3/2 * qb, sT**2/2 * qb],\n",
    "    [sT**3/2 * qb, sT**2 * qb, sT * qb],\n",
    "    [sT**2/2 * qb, sT * qb, qb],\n",
    "])\n",
    "\n",
    "# MEASUREMENT COVARIANCE R\n",
    "dlc_cov = 5**2\n",
    "\n",
    "# State prediction function jacobian F - shape: (n_states, n_states)\n",
    "rng = np.arange(n_states - vel_idx)\n",
    "rng_acc = np.arange(n_states - acc_idx)\n",
    "F = np.eye(n_states)\n",
    "F[rng, rng+vel_idx] = sT\n",
    "F[rng_acc, rng_acc+acc_idx] = sT**2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EKF & Smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space for storing EKF data\n",
    "states_est_hist = np.zeros((n_frames, n_states))\n",
    "states_pred_hist = states_est_hist.copy()\n",
    "P_est_hist = np.zeros((n_frames, n_states, n_states))\n",
    "P_pred_hist = P_est_hist.copy()\n",
    "\n",
    "outliers_ignored = 0\n",
    "\n",
    "for i in range(n_frames):\n",
    "    print(f\"Running frame {i+start_frame+1}\\r\", end='')\n",
    "    \n",
    "    # ========== PREDICTION ==========\n",
    "\n",
    "    # Predict State\n",
    "    states = predict_next_state(states, sT).flatten()\n",
    "    states_pred_hist[i] = states\n",
    "\n",
    "    # Projection of the state covariance\n",
    "    P = F @ P @ F.T + Q\n",
    "    P_pred_hist[i] = P\n",
    "    \n",
    "    # ============ UPDATE ============\n",
    "    \n",
    "    z_k = pixels_arr[i+start_frame]\n",
    "    likelihood = likelihood_arr[i+start_frame]\n",
    "    \n",
    "    # Measurement\n",
    "    H = np.zeros((n_cams*n_markers*2, n_states))\n",
    "    h = np.zeros((n_cams*n_markers*2)) # same as H[:, 0].copy()\n",
    "    for j in range(n_cams):\n",
    "        # State measurement\n",
    "        h[j*n_markers*2:(j+1)*n_markers*2] = h_function(states[:vel_idx], *camera_params[j]).flatten()\n",
    "        # Jacobian - shape: (2*n_markers, n_states)\n",
    "        H[j*n_markers*2:(j+1)*n_markers*2, 0:vel_idx] = numerical_jacobian(h_function, states[:vel_idx], *camera_params[j])\n",
    "    \n",
    "    # Measurement Covariance R\n",
    "    bad_point_mask = np.repeat(likelihood<dlc_thresh, 2)\n",
    "    dlc_cov_arr = dlc_cov*np.ones((n_cams*n_markers*2))\n",
    "    dlc_cov_arr[bad_point_mask] = max_pixel_err # change this to be independent of cam res?\n",
    "    R = np.diag(dlc_cov_arr**2)\n",
    "\n",
    "    # Residual\n",
    "    residual = z_k - h\n",
    "\n",
    "    # Residual Covariance S\n",
    "    S = (H @ P @ H.T) + R\n",
    "    temp = sigma_bound*np.sqrt(np.diag(S)) # if measurement residual is worse than 3 sigma, set residual to 0 and rely on predicted state only\n",
    "    for j in range(0, len(residual), 2):\n",
    "        if np.abs(residual[j])>temp[j] or np.abs(residual[j+1])>temp[j+1]:\n",
    "            residual[j:j+2] = 0\n",
    "            outliers_ignored += 1\n",
    "        \n",
    "    # Kalman Gain\n",
    "    K = P @ H.T @ np.linalg.inv(S)\n",
    "\n",
    "    # Correction\n",
    "    states = states + K @ residual\n",
    "    states_est_hist[i] = states\n",
    "\n",
    "    # Update State Covariance\n",
    "    P = (np.eye(K.shape[0]) - K @ H) @ P\n",
    "    P_est_hist[i] = P\n",
    "    \n",
    "print(\"Outliers ignored:\", outliers_ignored)\n",
    "\n",
    "# Run Kalman Smoother\n",
    "smooth_states_est_hist = states_est_hist.copy()\n",
    "smooth_P_est_hist = P_est_hist.copy()\n",
    "for i in range(n_frames-2, 0, -1):\n",
    "    A = P_est_hist[i] @ F.T @ np.linalg.inv(P_pred_hist[i+1])\n",
    "    smooth_states_est_hist[i] = states_est_hist[i] + A @ (smooth_states_est_hist[i+1] - states_pred_hist[i+1])\n",
    "    smooth_P_est_hist[i] = P_est_hist[i] + A @ (smooth_P_est_hist[i+1] - P_pred_hist[i+1]) @ A.T\n",
    "    \n",
    "print(\"\\nKalman Smoother complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save EKF results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ekf(states_est_hist, smooth_states_est_hist, DATA_DIR, start_frame, include_lure, dlc_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(8, 2, figsize=(13,30))\n",
    "# Head\n",
    "axs[0,0].plot(states_est_hist[:, [idx['x_0'], idx['y_0'], idx['z_0']]])\n",
    "axs[0,0].plot(smooth_states_est_hist[:, [idx['x_0'], idx['y_0'], idx['z_0']]], linestyle='dashed')\n",
    "axs[0,0].set_title(\"Head positions\")\n",
    "axs[0,0].legend(['x 0', 'y 0', 'z 0', 'x 0 (smoothed)', 'y 0 (smoothed)', 'z 0 (smoothed)'])\n",
    "\n",
    "# Head\n",
    "# axs[0,1].plot(states_est_hist[:, [d_x_0, d_y_0, d_z_0]])\n",
    "# axs[0,1].plot(smooth_states_est_hist[:, [d_x_0, d_y_0, d_z_0]], linestyle='dashed')\n",
    "# axs[0,1].set_title(\"Head velocity\")\n",
    "# axs[0,1].legend(['dx 0', 'dy 0', 'dz 0', 'dx 0 (smoothed)', 'dy 0 (smoothed)', 'dz 0 (smoothed)'])\n",
    "\n",
    "axs[1,0].plot(states_est_hist[:, [idx['phi_0'], idx['theta_0'], idx['psi_0']]])\n",
    "axs[1,0].plot(smooth_states_est_hist[:, [idx['phi_0'], idx['theta_0'], idx['psi_0']]], linestyle='dashed')\n",
    "axs[1,0].set_title(\"Head angles\")\n",
    "axs[1,0].legend(['phi 0', 'theta 0', 'psi 0', 'phi 0 (smoothed)', 'theta 0 (smoothed)', 'psi 0 (smoothed)'])\n",
    "\n",
    "axs[1,1].plot(states_est_hist[:, [phi_1, theta_1, psi_1]])\n",
    "axs[1,1].plot(smooth_states_est_hist[:, [phi_1, theta_1, psi_1]], linestyle='dashed')\n",
    "axs[1,1].set_title(\"Neck angles\")\n",
    "axs[1,1].legend(['phi 1', 'theta 1', 'psi 1', 'phi 1 (smoothed)', 'theta 1 (smoothed)', 'psi 1 (smoothed)'])\n",
    "\n",
    "axs[2,0].plot(states_est_hist[:, [theta_2]])\n",
    "axs[2,0].plot(smooth_states_est_hist[:, [theta_2]], linestyle='dashed')\n",
    "axs[2,0].set_title(\"Front torso angles\")\n",
    "axs[2,0].legend(['theta_2', 'theta_2 (smoothed)'])\n",
    "\n",
    "axs[2,1].plot(states_est_hist[:, [phi_3, theta_3, psi_3]])\n",
    "axs[2,1].plot(smooth_states_est_hist[:, [phi_3, theta_3, psi_3]], linestyle='dashed')\n",
    "axs[2,1].set_title(\"Back torso angles\")\n",
    "axs[2,1].legend(['phi_3', 'theta_3', 'psi_3', 'phi_3 (smoothed)', 'theta_3 (smoothed)', 'psi_3 (smoothed)'])\n",
    "\n",
    "axs[3,0].plot(states_est_hist[:, [theta_4, psi_4]])\n",
    "axs[3,0].plot(smooth_states_est_hist[:, [theta_4, psi_4]], linestyle='dashed')\n",
    "axs[3,0].set_title(\"Tail base\")\n",
    "axs[3,0].legend(['theta_4', 'psi_4','theta_4 (smoothed)', 'psi_4 (smoothed)'])\n",
    "\n",
    "axs[3,1].plot(states_est_hist[:, [theta_5, psi_5]])\n",
    "axs[3,1].plot(smooth_states_est_hist[:, [theta_5, psi_5]], linestyle='dashed')\n",
    "axs[3,1].set_title(\"Tail Mid\")\n",
    "axs[3,1].legend(['theta_5', 'psi_5', 'theta_5 (smoothed)', 'psi_5 (smoothed)'])\n",
    "\n",
    "axs[4,0].plot(states_est_hist[:, [theta_6]])\n",
    "axs[4,0].plot(smooth_states_est_hist[:, [theta_6]], linestyle='dashed')\n",
    "axs[4,0].set_title(\"Left shoulder angles\")\n",
    "axs[4,0].legend(['theta_6', 'theta_6 (smoothed)'])\n",
    "\n",
    "axs[4,1].plot(states_est_hist[:, [theta_7]])\n",
    "axs[4,1].plot(smooth_states_est_hist[:, [theta_7]], linestyle='dashed')\n",
    "axs[4,1].set_title(\"Left front knee angle\")\n",
    "axs[4,1].legend(['theta_7', 'theta_7 (smoothed)'])\n",
    "\n",
    "axs[5,0].plot(states_est_hist[:, [theta_8]])\n",
    "axs[5,0].plot(smooth_states_est_hist[:, [theta_8]], linestyle='dashed')\n",
    "axs[5,0].set_title(\"Right shoulder angles\")\n",
    "axs[5,0].legend(['theta_8', 'theta_8 (smoothed)'])\n",
    "\n",
    "axs[5,1].plot(states_est_hist[:, [theta_9]])\n",
    "axs[5,1].plot(smooth_states_est_hist[:, [theta_9]], linestyle='dashed')\n",
    "axs[5,1].set_title(\"Right front knee angle\")\n",
    "axs[5,1].legend(['theta_9', 'theta_9 (smoothed)'])\n",
    "\n",
    "axs[6,0].plot(states_est_hist[:, [theta_10]])\n",
    "axs[6,0].plot(smooth_states_est_hist[:, [theta_10]], linestyle='dashed')\n",
    "axs[6,0].set_title(\"Left hip angle\")\n",
    "axs[6,0].legend(['theta_10', 'theta_10 (smoothed)'])\n",
    "\n",
    "axs[6,1].plot(states_est_hist[:, [theta_11]])\n",
    "axs[6,1].plot(smooth_states_est_hist[:, [theta_11]], linestyle='dashed')\n",
    "axs[6,1].set_title(\"Left back knee angle\")\n",
    "axs[6,1].legend(['theta_11', 'theta_11 (smoothed)'])\n",
    "\n",
    "axs[7,0].plot(states_est_hist[:, [theta_12]])\n",
    "axs[7,0].plot(smooth_states_est_hist[:, [theta_12]], linestyle='dashed')\n",
    "axs[7,0].set_title(\"Right hip angle\")\n",
    "axs[7,0].legend(['theta_12', 'theta_12 (smoothed)'])\n",
    "\n",
    "axs[7,1].plot(states_est_hist[:, [theta_13]])\n",
    "axs[7,1].plot(smooth_states_est_hist[:, [theta_13]], linestyle='dashed')\n",
    "axs[7,1].set_title(\"Right back knee angle\")\n",
    "axs[7,1].legend(['theta_13', 'theta_13 (smoothed)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"ekf\", 'ekf_with_lure.pickle'), 'rb') as f:\n",
    "    ekf_data = pickle.load(f)\n",
    "    \n",
    "positions = ekf_data[\"smooth_positions\"]\n",
    "# a = PoseAnimation(positions)\n",
    "# a.animation()\n",
    "ca = plotting.Cheetah(positions, DATA_DIR,dark_mode=True)\n",
    "ca.animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
